{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5667e0f5",
   "metadata": {},
   "source": [
    "## üìö Quick demo notebook ‚Äì Bayesian Optimization with risk of non-convergence\n",
    "\n",
    "This short notebook shows how to combine  \n",
    "* **PyTorch** ‚Äì tensor backend  \n",
    "* **GPyTorch** ‚Äì Gaussian-process models  \n",
    "* **BoTorch** ‚Äì Bayesian-optimization utilities  \n",
    "* **Weights & Biases** ‚Äì experiment tracking  \n",
    "\n",
    "### What happens?\n",
    "\n",
    "1. **Two surrogate models** are created  \n",
    "   * `gp_reward`  ‚Äì *regression* GP for the objective value  \n",
    "   * `gp_conv`    ‚Äì *classification* GP (Bernoulli likelihood) for the probability that a run converges  \n",
    "\n",
    "2. They are wrapped in a **`ModelListGP`** container so we can query a joint posterior in a single call.\n",
    "\n",
    "3. We build an **acquisition function**  \n",
    "\n",
    "A(x) = ExpectedImprovement_reward(x) √ó P_convergence(x)\n",
    "\n",
    "This favors points that promise a high improvement **and** have a decent chance to finish successfully.\n",
    "\n",
    "4. `optimize_acqf` searches the acquisition landscape and proposes `q = 3` new hyper-parameter configurations.\n",
    "\n",
    "5. For each proposed config we launch *k* replicas, log `(converged?, reward)`, and update the surrogates ‚Äì the classic BO loop.\n",
    "\n",
    "> **Goal of the notebook:** verify that the two-head ModelList works,\n",
    "> inspect EI √ó Pconv visually, and ensure the code runs before plugging in your real training loop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16d25fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gpytorch\n",
    "from botorch.models import SingleTaskGP\n",
    "from botorch.models.model_list_gp_regression import ModelListGP\n",
    "from botorch.acquisition.analytic import ExpectedImprovement\n",
    "from botorch.optim import optimize_acqf\n",
    "import wandb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f1a02e49",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Logit' from 'botorch.models.transforms.outcome' (C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python313\\site-packages\\botorch\\models\\transforms\\outcome.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mbotorch\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mbotorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtransforms\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01moutcome\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Logit\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'Logit' from 'botorch.models.transforms.outcome' (C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python313\\site-packages\\botorch\\models\\transforms\\outcome.py)"
     ]
    }
   ],
   "source": [
    "import botorch\n",
    "from botorch.models.transforms.outcome import Logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c019659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Donn√©es binaires (0 / 1).  eps √©vite logit(0) ou logit(1)\n",
    "X        = torch.rand(40, 2)\n",
    "Y_raw    = (torch.rand(40) < 0.7).float().unsqueeze(-1)  # (40 √ó 1)\n",
    "eps      = 1e-3\n",
    "Y_prob   = Y_raw * (1 - 2*eps) + eps                     # ‚àà (0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2c419565",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_2328\\2690184970.py:1: InputDataWarning: The model inputs are of type torch.float32. It is strongly recommended to use double precision in BoTorch, as this improves both precision and stability and can help avoid numerical errors. See https://github.com/pytorch/botorch/discussions/1444\n",
      "  gp_conv = SingleTaskGP(\n"
     ]
    }
   ],
   "source": [
    "gp_conv = SingleTaskGP(\n",
    "    train_X = X,\n",
    "    train_Y = Y_prob,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a8faa8ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_2328\\2727844786.py:1: InputDataWarning: The model inputs are of type torch.float32. It is strongly recommended to use double precision in BoTorch, as this improves both precision and stability and can help avoid numerical errors. See https://github.com/pytorch/botorch/discussions/1444\n",
      "  gp_reward = SingleTaskGP(\n"
     ]
    }
   ],
   "source": [
    "gp_reward = SingleTaskGP(\n",
    "    train_X = X,\n",
    "    train_Y = Y_prob,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aacc3020",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ModelListGP(gp_reward, gp_conv)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
