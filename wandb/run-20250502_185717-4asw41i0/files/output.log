🚀 Starting training on device: mps for 50 epochs
Training: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:05<00:00,  8.71epoch/s, batch_max_reward=596, best_reward=647, loss=0.938]


🏁 Training finished
✨ Best reward: 647.390625
🧩 Best sequence: [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0]
-------------
647.3906249500001
647.3906
445.99999995
446.0
-------------
[1 1 1 1 0 1 1 1 1 0 0 0 1 0 0 1 0 0 0 1]
[1. 0. 0. 0. 1. 1. 1. 0. 1. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0.]
-------------
[2.8242188  0.         0.         0.         6.76953125 8.7421875
 9.9453125  8.8125     6.9296875  6.94921875 8.4609375  9.359375
 9.859375   8.8984375  8.03125    8.2109375  9.703125   9.359375
 8.3984375  6.28515625]
[89. 17. 91. 17. 44. 80. 82. 67. 21. 38. 29. 87. 82. 68. 42. 89. 22. 55.
 42. 74.]
Iteration 1 → Master: 446.000000, Follower(seq): 647.390625, Follower(analytic): 690.003906, Gap: +201.390625
→ Adding new constraint from follower.
🚀 Starting training on device: mps for 50 epochs
Training: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:04<00:00, 10.30epoch/s, batch_max_reward=623, best_reward=648, loss=0.882]


🏁 Training finished
✨ Best reward: 648.0546875
🧩 Best sequence: [1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0]
-------------
648.0546875
648.0547
669.0
669.0
-------------
[1 1 1 0 0 1 1 1 0 1 0 0 1 0 0 1 0 0 0 1]
[1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1.]
-------------
[0.         0.         2.140625   0.         6.76953125 8.7421875
 9.9453125  8.8125     6.9296875  6.94921875 8.4609375  9.359375
 9.859375   8.8984375  8.03125    8.2109375  9.703125   9.359375
 8.3984375  6.28515625]
[89. 17. 91. 17. 44. 80. 82. 67. 21. 38. 29. 87. 82. 68. 42. 89. 22. 55.
 42. 74.]
Iteration 2 → Master: 669.000000, Follower(seq): 648.054688, Follower(analytic): 690.687500, Gap: -20.945312
🚀 Starting training on device: mps for 50 epochs
Training: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:04<00:00, 10.23epoch/s, batch_max_reward=567, best_reward=674, loss=1.12]


🏁 Training finished
✨ Best reward: 674.47265625
🧩 Best sequence: [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0]
-------------
674.47265625
674.47266
669.0
669.0
-------------
[1 1 1 1 0 1 0 0 0 0 0 1 1 1 1 1 0 0 0 1]
[1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1.]
-------------
[0.         0.         2.140625   0.         6.76953125 8.7421875
 9.9453125  8.8125     6.9296875  6.94921875 8.4609375  9.359375
 9.859375   8.8984375  8.03125    8.2109375  9.703125   9.359375
 8.3984375  6.28515625]
[89. 17. 91. 17. 44. 80. 82. 67. 21. 38. 29. 87. 82. 68. 42. 89. 22. 55.
 42. 74.]
Iteration 3 → Master: 669.000000, Follower(seq): 674.472656, Follower(analytic): 690.687500, Gap: +5.472656
→ Adding new constraint from follower.
🚀 Starting training on device: mps for 50 epochs
Training: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:04<00:00, 10.24epoch/s, batch_max_reward=625, best_reward=675, loss=1.26]


🏁 Training finished
✨ Best reward: 674.9140625
🧩 Best sequence: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0]
-------------
674.9140625
674.91406
684.0
684.0
-------------
[1 1 1 1 0 0 1 1 0 0 0 1 0 1 0 1 0 1 0 1]
[1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 1. 0. 0. 0. 0.]
-------------
[0.         0.         0.21484375 0.         6.76953125 8.7421875
 9.9453125  8.8125     6.9296875  6.94921875 8.4609375  9.359375
 9.859375   8.8984375  8.03125    8.2109375  9.703125   9.359375
 8.3984375  6.28515625]
[89. 17. 91. 17. 44. 80. 82. 67. 21. 38. 29. 87. 82. 68. 42. 89. 22. 55.
 42. 74.]
Iteration 4 → Master: 684.000000, Follower(seq): 674.914062, Follower(analytic): 714.484375, Gap: -9.085938
🚀 Starting training on device: mps for 50 epochs
Training: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:04<00:00, 10.26epoch/s, batch_max_reward=586, best_reward=659, loss=0.796]


🏁 Training finished
✨ Best reward: 659.3125
🧩 Best sequence: [1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0]
-------------
659.3125
659.3125
684.0
684.0
-------------
[1 0 1 1 0 1 1 0 0 0 0 1 0 1 1 1 0 0 0 1]
[1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 1. 0. 0. 0. 0.]
-------------
[0.         0.         0.21484375 0.         6.76953125 8.7421875
 9.9453125  8.8125     6.9296875  6.94921875 8.4609375  9.359375
 9.859375   8.8984375  8.03125    8.2109375  9.703125   9.359375
 8.3984375  6.28515625]
[89. 17. 91. 17. 44. 80. 82. 67. 21. 38. 29. 87. 82. 68. 42. 89. 22. 55.
 42. 74.]
Iteration 5 → Master: 684.000000, Follower(seq): 659.312500, Follower(analytic): 714.484375, Gap: -24.687500
🚀 Starting training on device: mps for 50 epochs
Training: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:05<00:00,  9.84epoch/s, batch_max_reward=633, best_reward=658, loss=1.04]


🏁 Training finished
✨ Best reward: 658.0
🧩 Best sequence: [1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0]
-------------
658.0
658.0
684.0
684.0
-------------
[1 0 1 1 0 0 0 1 0 0 0 1 1 1 0 1 0 1 0 1]
[1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 1. 0. 0. 0. 0.]
-------------
[0.         0.         0.21484375 0.         6.76953125 8.7421875
 9.9453125  8.8125     6.9296875  6.94921875 8.4609375  9.359375
 9.859375   8.8984375  8.03125    8.2109375  9.703125   9.359375
 8.3984375  6.28515625]
[89. 17. 91. 17. 44. 80. 82. 67. 21. 38. 29. 87. 82. 68. 42. 89. 22. 55.
 42. 74.]
Iteration 6 → Master: 684.000000, Follower(seq): 658.000000, Follower(analytic): 714.484375, Gap: -26.000000
🚀 Starting training on device: mps for 50 epochs
Training: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:05<00:00,  9.94epoch/s, batch_max_reward=618, best_reward=666, loss=0.895]


🏁 Training finished
✨ Best reward: 666.0859375
🧩 Best sequence: [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0]
-------------
666.0859375
666.08594
684.0
684.0
-------------
[1 0 1 1 1 1 1 1 0 0 0 1 1 0 0 1 0 0 0 0]
[1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 1. 0. 0. 0. 0.]
-------------
[0.         0.         0.21484375 0.         6.76953125 8.7421875
 9.9453125  8.8125     6.9296875  6.94921875 8.4609375  9.359375
 9.859375   8.8984375  8.03125    8.2109375  9.703125   9.359375
 8.3984375  6.28515625]
[89. 17. 91. 17. 44. 80. 82. 67. 21. 38. 29. 87. 82. 68. 42. 89. 22. 55.
 42. 74.]
Iteration 7 → Master: 684.000000, Follower(seq): 666.085938, Follower(analytic): 714.484375, Gap: -17.914062
🚀 Starting training on device: mps for 50 epochs
Training:  40%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                                                                            | 20/50 [00:02<00:03,  9.59epoch/s, batch_max_reward=663, best_reward=714, loss=0.831]
Traceback (most recent call last):
  File "/Users/arthur/Library/Mobile Documents/com~apple~CloudDocs/Cours/ECN6338/Devoir à rendre/projet Avec critique/train.py", line 330, in <module>
    master_val, follower_val, iteration = solve(u_vals, t_vals, B_val, cfg)
                                          ~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/arthur/Library/Mobile Documents/com~apple~CloudDocs/Cours/ECN6338/Devoir à rendre/projet Avec critique/train.py", line 154, in solve
    follower_val_t, x_hat_sol = train_model(u_vals=u, t_vals=t_sol, B_val=B, epochs=cfg.num_epochs, batch_size=batch_size, actor=actor, critic=critic, optimizer_ac=optimizer_ac, optimizer_cr=optimizer_cr, device=device)
                                ~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/arthur/Library/Mobile Documents/com~apple~CloudDocs/Cours/ECN6338/Devoir à rendre/projet Avec critique/train.py", line 244, in train_model
    loss.backward()
    ~~~~~~~~~~~~~^^
  File "/Users/arthur/miniconda3/envs/GFlowNet-Knapsack-Actor-Critic/lib/python3.13/site-packages/torch/_tensor.py", line 648, in backward
    torch.autograd.backward(
    ~~~~~~~~~~~~~~~~~~~~~~~^
        self, gradient, retain_graph, create_graph, inputs=inputs
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/arthur/miniconda3/envs/GFlowNet-Knapsack-Actor-Critic/lib/python3.13/site-packages/torch/autograd/__init__.py", line 353, in backward
    _engine_run_backward(
    ~~~~~~~~~~~~~~~~~~~~^
        tensors,
        ^^^^^^^^
    ...<5 lines>...
        accumulate_grad=True,
        ^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/arthur/miniconda3/envs/GFlowNet-Knapsack-Actor-Critic/lib/python3.13/site-packages/torch/autograd/graph.py", line 824, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        t_outputs, *args, **kwargs
        ^^^^^^^^^^^^^^^^^^^^^^^^^^
    )  # Calls into the C++ engine to run the backward pass
    ^
KeyboardInterrupt
Exception ignored in atexit callback <function _start_and_connect_service.<locals>.teardown_atexit at 0x10c08bf60>:
Traceback (most recent call last):
  File "/Users/arthur/miniconda3/envs/GFlowNet-Knapsack-Actor-Critic/lib/python3.13/site-packages/wandb/sdk/lib/service_connection.py", line 94, in teardown_atexit
    conn.teardown(hooks.exit_code)
  File "/Users/arthur/miniconda3/envs/GFlowNet-Knapsack-Actor-Critic/lib/python3.13/site-packages/wandb/sdk/lib/service_connection.py", line 226, in teardown
    self._router.join()
  File "/Users/arthur/miniconda3/envs/GFlowNet-Knapsack-Actor-Critic/lib/python3.13/site-packages/wandb/sdk/interface/router.py", line 75, in join
    self._thread.join()
  File "/Users/arthur/miniconda3/envs/GFlowNet-Knapsack-Actor-Critic/lib/python3.13/threading.py", line 1092, in join
    self._handle.join(timeout)
KeyboardInterrupt:
