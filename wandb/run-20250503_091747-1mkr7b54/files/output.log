🚀 Starting training on device: mps for 50 epochs
Training: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:04<00:00, 10.39epoch/s, batch_max_reward=435, best_reward=506, loss=1.1]


🏁 Training finished
✨ Best reward: 505.73046875
🧩 Best sequence: [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0]
-------------
505.73046875
505.73047
275.0
275.0
-------------
[1 1 1 0 1 1 1 0 0 0 1 0 1 0 0]
[0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 1. 1.]
-------------
[0.         7.8984375  0.         8.3359375  9.9453125  6.3125
 7.16015625 8.0546875  9.765625   7.921875   6.9765625  8.7421875
 6.9765625  6.1953125  8.953125  ]
[74. 93. 27. 25. 99. 25. 51. 51. 83. 18. 88. 10. 94. 25. 40.]
Iteration 1 → Master: 275.000000, Follower(seq): 505.730469, Gap: +230.730469
→ Adding new constraint from follower.
🚀 Starting training on device: mps for 50 epochs
Training: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:03<00:00, 12.77epoch/s, batch_max_reward=502, best_reward=564, loss=0.914]


🏁 Training finished
✨ Best reward: 563.8984375
🧩 Best sequence: [1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0]
-------------
563.8984375
563.89844
525.0
525.0
-------------
[1 1 1 0 1 0 0 1 1 0 1 0 1 0 0]
[1. 1. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 1. 0. 1.]
-------------
[0.         3.3828125  0.         8.3359375  9.9453125  6.3125
 7.16015625 8.0546875  9.765625   7.921875   6.9765625  8.7421875
 6.9765625  6.1953125  8.953125  ]
[74. 93. 27. 25. 99. 25. 51. 51. 83. 18. 88. 10. 94. 25. 40.]
Iteration 2 → Master: 525.000000, Follower(seq): 563.898438, Gap: +38.898438
→ Adding new constraint from follower.
🚀 Starting training on device: mps for 50 epochs
Training: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:03<00:00, 12.96epoch/s, batch_max_reward=494, best_reward=565, loss=0.743]


🏁 Training finished
✨ Best reward: 564.79296875
🧩 Best sequence: [1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0]
-------------
564.79296875
564.79297
563.8984375
563.89844
-------------
[1 1 1 0 1 0 1 0 1 0 1 0 1 0 0]
[1. 1. 1. 0. 1. 0. 0. 1. 1. 0. 1. 0. 1. 0. 0.]
-------------
[0.         3.3828125  0.         8.3359375  9.9453125  6.3125
 7.16015625 8.0546875  9.765625   7.921875   6.9765625  8.7421875
 6.9765625  6.1953125  8.953125  ]
[74. 93. 27. 25. 99. 25. 51. 51. 83. 18. 88. 10. 94. 25. 40.]
Iteration 3 → Master: 563.898438, Follower(seq): 564.792969, Gap: +0.894531
→ Adding new constraint from follower.
🚀 Starting training on device: mps for 50 epochs
Training: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:03<00:00, 13.19epoch/s, batch_max_reward=440, best_reward=541, loss=1.5]


🏁 Training finished
✨ Best reward: 540.65234375
🧩 Best sequence: [1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0]
-------------
540.6523437999999
540.65234
564.7929687999999
564.79297
-------------
[1 1 1 0 1 0 0 0 1 0 1 0 1 1 0]
[1. 1. 1. 0. 1. 0. 0. 1. 1. 0. 1. 0. 1. 0. 0.]
-------------
[0.         2.4882812  0.         8.3359375  9.9453125  6.3125
 7.16015625 8.0546875  9.765625   7.921875   6.9765625  8.7421875
 6.9765625  6.1953125  8.953125  ]
[74. 93. 27. 25. 99. 25. 51. 51. 83. 18. 88. 10. 94. 25. 40.]
Iteration 4 → Master: 564.792969, Follower(seq): 540.652344, Gap: -24.140625
🚀 Starting training on device: mps for 50 epochs
Training:  40%|███████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                                                                              | 20/50 [00:01<00:02, 12.70epoch/s, batch_max_reward=466, best_reward=523, loss=1.05]
Traceback (most recent call last):
  File "/Users/arthur/Library/Mobile Documents/com~apple~CloudDocs/Cours/ECN6338/Devoir à rendre/projet Avec critique/train.py", line 351, in <module>
    master_val, follower_val, iteration, start_time = solve(u_vals, t_vals, B_val, cfg)
                                                      ~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/arthur/Library/Mobile Documents/com~apple~CloudDocs/Cours/ECN6338/Devoir à rendre/projet Avec critique/train.py", line 157, in solve
    follower_val_t, x_hat_sol = train_model(u_vals=u, t_vals=t_sol, B_val=B, epochs=cfg.num_epochs, batch_size=batch_size, actor=actor, critic=critic, optimizer_ac=optimizer_ac, optimizer_cr=optimizer_cr, device=device)
                                ~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/arthur/Library/Mobile Documents/com~apple~CloudDocs/Cours/ECN6338/Devoir à rendre/projet Avec critique/train.py", line 253, in train_model
    logp_cand, selected_cand = actor.generate_trajectories(
                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~^
        B_tensor.expand(batch_size, 1),
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    ...<4 lines>...
        device,
        ^^^^^^^
    )
    ^
  File "/Users/arthur/Library/Mobile Documents/com~apple~CloudDocs/Cours/ECN6338/Devoir à rendre/projet Avec critique/models/GFlowNet_v1.py", line 89, in generate_trajectories
    logp_acc += dist.log_prob(act)
                ~~~~~~~~~~~~~^^^^^
  File "/Users/arthur/miniconda3/envs/GFlowNet-Knapsack-Actor-Critic/lib/python3.13/site-packages/torch/distributions/bernoulli.py", line 112, in log_prob
    logits, value = broadcast_all(self.logits, value)
                                  ^^^^^^^^^^^
  File "/Users/arthur/miniconda3/envs/GFlowNet-Knapsack-Actor-Critic/lib/python3.13/site-packages/torch/distributions/utils.py", line 165, in __get__
    value = self.wrapped(instance)
  File "/Users/arthur/miniconda3/envs/GFlowNet-Knapsack-Actor-Critic/lib/python3.13/site-packages/torch/distributions/bernoulli.py", line 94, in logits
    return probs_to_logits(self.probs, is_binary=True)
  File "/Users/arthur/miniconda3/envs/GFlowNet-Knapsack-Actor-Critic/lib/python3.13/site-packages/torch/distributions/utils.py", line 131, in probs_to_logits
    return torch.log(ps_clamped) - torch.log1p(-ps_clamped)
           ~~~~~~~~~^^^^^^^^^^^^
KeyboardInterrupt
